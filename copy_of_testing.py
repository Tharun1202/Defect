# -*- coding: utf-8 -*-
"""Copy of Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bCf6k8PYcaFjCS4zQBH1KTScJusoUWxN
"""

# Install Hugging Face Transformers and Datasets libraries
!pip install transformers datasets



# Clone the GitHub repository again if needed
!git clone https://github.com/jkoppel/QuixBugs.git

# List the contents of the QuixBugs directory
!ls QuixBugs

# Set the paths to the directories you need
java_program_dir = './QuixBugs/java_programs'
correct_program_dir = './QuixBugs/correct_java_programs'

import os
import pandas as pd

# Initialize lists to hold the code snippets and their labels
code_snippets = []
labels = []

# Function to read Java files and label them accordingly
def read_java_files(directory, label):
    for filename in os.listdir(directory):
        if filename.endswith(".java"):
            filepath = os.path.join(directory, filename)
            with open(filepath, 'r', encoding='utf-8') as file:
                code = file.read()
                code_snippets.append(code)
                labels.append(label)

# Read incorrect Java files from 'java_program' (label = 1 for buggy)
read_java_files(java_program_dir, 1)

# Read correct Java files from 'correct_java_program' (label = 0 for correct)
read_java_files(correct_program_dir, 0)

# Create a DataFrame with the code snippets and their labels
df = pd.DataFrame({
    'code': code_snippets,
    'label': labels
})

# Display the DataFrame to check the content
df.head()

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=df)

from transformers import RobertaTokenizer

# Load the CodeBERT tokenizer
tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")

# Tokenize the code snippets
def tokenize_function(examples):
    return tokenizer(examples['code'], padding="max_length", truncation=True, max_length=128)

# Apply tokenization to the DataFrame
df['tokenized'] = df['code'].apply(lambda x: tokenizer(x, padding="max_length", truncation=True, max_length=128))

!pip install datasets

from datasets import Dataset

# Convert the DataFrame with raw code and labels to a Dataset
dataset = Dataset.from_pandas(df[['code', 'label']])

# Split the dataset into training and validation sets (80/20 split)
train_test_split = dataset.train_test_split(test_size=0.2)
train_dataset = train_test_split['train']
val_dataset = train_test_split['test']

def tokenize_function(example):
    return tokenizer(example["code"], padding="max_length", truncation=True)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

print(tokenized_dataset.column_names)

'''from transformers import Trainer, TrainingArguments, RobertaForSequenceClassification

# Load the model for sequence classification
model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=2)

# Define the training arguments
training_args = TrainingArguments(
    output_dir='./results',       # Output directory
    evaluation_strategy="epoch",  # Evaluate each epoch
    learning_rate=2e-5,           # Learning rate
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,           # Train for 3 epochs
    weight_decay=0.01,            # Weight decay
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer  # Ensure the tokenizer is passed
)

# Fine-tune the model
trainer.train()
'''

for example in tokenized_dataset:
    input_ids = example['input_ids']
    print(input_ids)

from transformers import DataCollatorWithPadding

# Use the tokenizer to pad the inputs in each batch
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

def tokenize_function(example):
    return tokenizer(
        example["code"],
        padding="max_length",
        truncation=True,
        max_length=128  # Adjust max_length based on your needs
    )

# Tokenize the entire dataset
tokenized_dataset = dataset.map(tokenize_function, batched=True)

'''
from transformers import Trainer, TrainingArguments, RobertaForSequenceClassification

# Load model for sequence classification
model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=2)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',       # Output directory
    evaluation_strategy="epoch",  # Evaluate every epoch
    learning_rate=2e-5,           # Learning rate
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,           # Number of epochs
    weight_decay=0.01,            # Weight decay
)

# Initialize Trainer with the model, arguments, and data collator
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],  # Use the correct split
    eval_dataset=tokenized_dataset["validation"],  # Ensure this exists
    tokenizer=tokenizer,
    data_collator=data_collator  # Ensure the data is properly batched and padded
)'''

# Split the dataset into 80% train and 20% validation
split_dataset = tokenized_dataset.train_test_split(test_size=0.2)

# Now your dataset has 'train' and 'test' splits
print(split_dataset)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=split_dataset["train"],   # Use the training split
    eval_dataset=split_dataset["test"],     # Use the test (validation) split
    tokenizer=tokenizer,
    data_collator=data_collator
)

trainer.train()

# Evaluate the model
eval_results = trainer.evaluate()

# Print evaluation results (loss, accuracy, etc.)
print(f"Evaluation Results: {eval_results}")

# Extract accuracy from the evaluation results
accuracy = eval_results["eval_accuracy"]
print(f"Validation Accuracy: {accuracy * 100:.2f}%")

import numpy as np

from transformers import Trainer

# ... (Your existing code for model, tokenizer, data, etc.) ...

# Define a function to compute metrics
def compute_metrics(eval_pred):
    """Computes accuracy metric."""
    metric = evaluate.load("accuracy")  # Load the accuracy metric
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)  # Get predicted class labels
    return metric.compute(predictions=predictions, references=labels)

# ... (Your existing code for training arguments, data splitting, etc.) ...

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=split_dataset["train"],
    eval_dataset=split_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics  # Pass the metrics function
)

trainer.train()

eval_results = trainer.evaluate()
print(f"Evaluation Results: {eval_results}")

# Extract accuracy from the evaluation results
accuracy = eval_results["eval_accuracy"]  # This should now work
print(f"Validation Accuracy: {accuracy * 100:.2f}%")

!pip install evaluate

!pip install evaluate
import numpy as np
import evaluate # Import the evaluate module here

from transformers import Trainer

# ... (Your existing code for model, tokenizer, data, etc.) ...

# Define a function to compute metrics
def compute_metrics(eval_pred):
    """Computes accuracy metric."""
    metric = evaluate.load("accuracy")  # Load the accuracy metric
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)  # Get predicted class labels
    return metric.compute(predictions=predictions, references=labels)

# ... (Your existing code for training arguments, data splitting, etc.) ...

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=split_dataset["train"],
    eval_dataset=split_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics  # Pass the metrics function
)

trainer.train()

eval_results = trainer.evaluate()
print(f"Evaluation Results: {eval_results}")

# Extract accuracy from the evaluation results
accuracy = eval_results["eval_accuracy"]  # This should now work
print(f"Validation Accuracy: {accuracy * 100:.2f}%")

